{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6fe3d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ FaceMesh started - ESC to exit\n"
     ]
    }
   ],
   "source": [
    "# ì¢Œìš° / ì‹œì„ (face_mesh) ì¸¡ì •\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"âŒ Webcam not opened.\")\n",
    "    exit()\n",
    "\n",
    "print(\"ğŸ¥ FaceMesh started - ESC to exit\")\n",
    "\n",
    "# ëœë“œë§ˆí¬ ì¸ë±ìŠ¤\n",
    "LEFT_EYE_OUTER = 33\n",
    "RIGHT_EYE_OUTER = 263\n",
    "LEFT_IRIS_LEFT = 471\n",
    "RIGHT_IRIS_RIGHT = 474\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(rgb)\n",
    "\n",
    "    gaze_text = \"\"\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        face = result.multi_face_landmarks[0]\n",
    "        lm = face.landmark\n",
    "\n",
    "        # ì¢Œí‘œ ì¶”ì¶œ (í”½ì…€ ë‹¨ìœ„)\n",
    "        left_eye_outer = np.array([lm[LEFT_EYE_OUTER].x * w, lm[LEFT_EYE_OUTER].y * h])\n",
    "        left_iris_left = np.array([lm[LEFT_IRIS_LEFT].x * w, lm[LEFT_IRIS_LEFT].y * h])\n",
    "\n",
    "        right_eye_outer = np.array([lm[RIGHT_EYE_OUTER].x * w, lm[RIGHT_EYE_OUTER].y * h])\n",
    "        right_iris_right = np.array([lm[RIGHT_IRIS_RIGHT].x * w, lm[RIGHT_IRIS_RIGHT].y * h])\n",
    "\n",
    "        # ê±°ë¦¬ ê³„ì‚°\n",
    "        left_distance = np.linalg.norm(left_eye_outer - left_iris_left)\n",
    "        right_distance = np.linalg.norm(right_eye_outer - right_iris_right)\n",
    "\n",
    "        # ì‹œì„  íŒë³„ ê¸°ì¤€\n",
    "        if left_distance < 10:\n",
    "            gaze_text = \"ğŸ‘ï¸ Looking LEFT\"\n",
    "        elif right_distance < 10:\n",
    "            gaze_text = \"ğŸ‘ï¸ Looking RIGHT\"\n",
    "        else:\n",
    "            gaze_text = \"ğŸ‘ï¸ Looking CENTER\"\n",
    "\n",
    "        # ë””ë²„ê¹…ìš© ì  ì‹œê°í™”\n",
    "        for idx in [LEFT_EYE_OUTER, LEFT_IRIS_LEFT, RIGHT_EYE_OUTER, RIGHT_IRIS_RIGHT]:\n",
    "            cx = int(lm[idx].x * w)\n",
    "            cy = int(lm[idx].y * h)\n",
    "            cv2.circle(frame, (cx, cy), 3, (0, 255, 255), -1)\n",
    "\n",
    "    # ì¶œë ¥ í…ìŠ¤íŠ¸\n",
    "    cv2.putText(frame, gaze_text, (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Iris Gaze Tracking (Outer Iris Landmarks)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0c73b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ Tracking started - ESC to exit\n"
     ]
    }
   ],
   "source": [
    "# ì¢Œìš° / ì‹œì„ (face_mesh), ê³ ê°œ(face_mesh), ì–´ê¹¨(Pose) ì¸¡ì • \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "pose = mp_pose.Pose(static_image_mode=False)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"âŒ Webcam not opened.\")\n",
    "    exit()\n",
    "\n",
    "print(\"ğŸ¥ Tracking started - ESC to exit\")\n",
    "\n",
    "# ëœë“œë§ˆí¬ ì¸ë±ìŠ¤\n",
    "LEFT_EYE_OUTER = 33\n",
    "RIGHT_EYE_OUTER = 263\n",
    "LEFT_IRIS_LEFT = 471\n",
    "RIGHT_IRIS_RIGHT = 474\n",
    "LEFT_EAR = 234\n",
    "RIGHT_EAR = 454\n",
    "LEFT_EYE_CENTER = 468\n",
    "RIGHT_EYE_CENTER = 473\n",
    "\n",
    "LEFT_SHOULDER = mp_pose.PoseLandmark.LEFT_SHOULDER\n",
    "RIGHT_SHOULDER = mp_pose.PoseLandmark.RIGHT_SHOULDER\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    face_result = face_mesh.process(rgb)\n",
    "    pose_result = pose.process(rgb)\n",
    "\n",
    "    gaze_text, head_text, shoulder_text = \"\", \"\", \"\"\n",
    "\n",
    "    # ---------- 1. ì–¼êµ´ ë°©í–¥, ì‹œì„  ----------\n",
    "    if face_result.multi_face_landmarks:\n",
    "        face = face_result.multi_face_landmarks[0]\n",
    "        lm = face.landmark\n",
    "\n",
    "        # ì‹œì„  ë°©í–¥\n",
    "        left_eye_outer = np.array([lm[LEFT_EYE_OUTER].x * w, lm[LEFT_EYE_OUTER].y * h])\n",
    "        left_iris = np.array([lm[LEFT_IRIS_LEFT].x * w, lm[LEFT_IRIS_LEFT].y * h])\n",
    "        right_eye_outer = np.array([lm[RIGHT_EYE_OUTER].x * w, lm[RIGHT_EYE_OUTER].y * h])\n",
    "        right_iris = np.array([lm[RIGHT_IRIS_RIGHT].x * w, lm[RIGHT_IRIS_RIGHT].y * h])\n",
    "\n",
    "        left_dist = np.linalg.norm(left_eye_outer - left_iris)\n",
    "        right_dist = np.linalg.norm(right_eye_outer - right_iris)\n",
    "\n",
    "        if left_dist < 10:\n",
    "            gaze_text = \"ğŸ‘ï¸ Gaze: LEFT\"\n",
    "        elif right_dist < 10:\n",
    "            gaze_text = \"ğŸ‘ï¸ Gaze: RIGHT\"\n",
    "        else:\n",
    "            gaze_text = \"ğŸ‘ï¸ Gaze: CENTER\"\n",
    "\n",
    "        # ê³ ê°œ ë°©í–¥ (ì–‘ ëˆˆ â†” ê·€ ê±°ë¦¬ ë¹„êµ)\n",
    "        left_eye_center = lm[LEFT_EYE_CENTER]\n",
    "        right_eye_center = lm[RIGHT_EYE_CENTER]\n",
    "        left_ear = lm[LEFT_EAR]\n",
    "        right_ear = lm[RIGHT_EAR]\n",
    "\n",
    "        left_eye_to_ear = abs(left_eye_center.x - left_ear.x)\n",
    "        right_eye_to_ear = abs(right_eye_center.x - right_ear.x)\n",
    "\n",
    "        if left_eye_to_ear > right_eye_to_ear + 0.02:\n",
    "            head_text = \"ğŸ§  Head: RIGHT TURN\"\n",
    "        elif right_eye_to_ear > left_eye_to_ear + 0.02:\n",
    "            head_text = \"ğŸ§  Head: LEFT TURN\"\n",
    "        else:\n",
    "            head_text = \"ğŸ§  Head: CENTER\"\n",
    "\n",
    "        # ëˆˆ/í™ì±„ ì‹œê°í™”\n",
    "        for idx in [LEFT_EYE_OUTER, LEFT_IRIS_LEFT, RIGHT_EYE_OUTER, RIGHT_IRIS_RIGHT]:\n",
    "            cx, cy = int(lm[idx].x * w), int(lm[idx].y * h)\n",
    "            cv2.circle(frame, (cx, cy), 3, (0, 255, 255), -1)\n",
    "\n",
    "    # ---------- 2. ì–´ê¹¨ ìì„¸ ----------\n",
    "    if pose_result.pose_landmarks:\n",
    "        landmarks = pose_result.pose_landmarks.landmark\n",
    "        left_y = landmarks[LEFT_SHOULDER].y\n",
    "        right_y = landmarks[RIGHT_SHOULDER].y\n",
    "        diff = left_y - right_y\n",
    "\n",
    "        if diff > 0.018:\n",
    "            shoulder_text = \"ğŸ§ Shoulder: LEFT UP\"\n",
    "        elif diff < -0.018:\n",
    "            shoulder_text = \"ğŸ§ Shoulder: RIGHT UP\"\n",
    "        else:\n",
    "            shoulder_text = \"ğŸ§ Shoulder: STRAIGHT\"\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            pose_result.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=3)\n",
    "        )\n",
    "\n",
    "    # ---------- ì „ì²´ í…ìŠ¤íŠ¸ ì¶œë ¥ ----------\n",
    "    y0 = 30\n",
    "    for line in [gaze_text, head_text, shoulder_text]:\n",
    "        if line:\n",
    "            cv2.putText(frame, line, (10, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            y0 += 30\n",
    "\n",
    "    cv2.imshow(\"Full Tracker (Gaze + Head + Shoulder)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41615642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ Gaze ratio tracking started - ESC to exit\n"
     ]
    }
   ],
   "source": [
    "# ì‹œì„  ì¸¡ì •í• ë•Œ, ë’¤ë¡œê°€ë©´ ìˆ˜ì¹˜ê°€ ì‘ì•„ì§€ë‹ˆê¹Œ ì²˜ìŒ ifì¸ ì™¼ìª½ì´ ê³„ì† ë– ì„œ, ëˆˆë™ì ê¸¸ì´ ìƒê°í•´ì„œ ëˆˆ ì™¼ìª½ê³¼ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ ì˜†ì— ë”°ëŠ” ë°©ì‹ìœ¼ë¡œ í•´ë´„.\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# ëœë“œë§ˆí¬ ì¸ë±ìŠ¤\n",
    "LEFT_EYE_OUTER = 33\n",
    "LEFT_IRIS_LEFT = 471\n",
    "LEFT_IRIS_RIGHT = 469\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"âŒ Webcam not opened.\")\n",
    "    exit()\n",
    "\n",
    "print(\"ğŸ¥ Gaze ratio tracking started - ESC to exit\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(rgb)\n",
    "\n",
    "    gaze_text = \"\"\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        face = result.multi_face_landmarks[0]\n",
    "        lm = face.landmark\n",
    "\n",
    "        # í”½ì…€ ì¢Œí‘œ ì¶”ì¶œ\n",
    "        eye_outer = np.array([lm[LEFT_EYE_OUTER].x * w, lm[LEFT_EYE_OUTER].y * h])\n",
    "        iris_left = np.array([lm[LEFT_IRIS_LEFT].x * w, lm[LEFT_IRIS_LEFT].y * h])\n",
    "        iris_right = np.array([lm[LEFT_IRIS_RIGHT].x * w, lm[LEFT_IRIS_RIGHT].y * h])\n",
    "\n",
    "        iris_width = np.linalg.norm(iris_right - iris_left) + 1e-6  # ë‚˜ëˆ—ì…ˆ ì˜¤ë¥˜ ë°©ì§€\n",
    "        to_outer = np.linalg.norm(iris_left - eye_outer)\n",
    "\n",
    "        ratio = to_outer / iris_width\n",
    "\n",
    "        # ì„ê³„ê°’ ê¸°ì¤€ (0.48 ì´í•˜ë©´ ì™¼ìª½ ì‘ì‹œë¡œ íŒë‹¨)\n",
    "        if ratio < 0.48:\n",
    "            gaze_text = f\"ğŸ‘ï¸ LEFT EYE â†’ Looking LEFT ({ratio:.2f})\"\n",
    "        else:\n",
    "            gaze_text = f\"ğŸ‘ï¸ LEFT EYE â†’ CENTER/RIGHT ({ratio:.2f})\"\n",
    "\n",
    "        # ë””ë²„ê¹…ìš© ì  ì¶œë ¥\n",
    "        for idx in [LEFT_EYE_OUTER, LEFT_IRIS_LEFT, LEFT_IRIS_RIGHT]:\n",
    "            cx, cy = int(lm[idx].x * w), int(lm[idx].y * h)\n",
    "            cv2.circle(frame, (cx, cy), 3, (0, 255, 255), -1)\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ì¶œë ¥\n",
    "    cv2.putText(frame, gaze_text, (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Gaze Ratio (Left Eye)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062233c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ Tracking started - ESC to exit\n"
     ]
    }
   ],
   "source": [
    "# ì‹œì„ , ê³ ê°œ, ì–´ê¹¨ ë¶„ì„ (ì‹œì„  ë¹„ìœ¨ë¡œ í•´ì„œ)í•©ì³¤ëŠ”ë°, ê³ ê°œê°€ ì˜ ì•ˆëŒ -> í™•ì¸í•´ì•¼í•¨. \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "\n",
    "# ëœë“œë§ˆí¬ ì¸ë±ìŠ¤\n",
    "LEFT_EYE_OUTER = 33\n",
    "LEFT_IRIS_LEFT = 471\n",
    "LEFT_IRIS_RIGHT = 469\n",
    "\n",
    "RIGHT_EYE_OUTER = 263\n",
    "RIGHT_IRIS_LEFT = 476\n",
    "RIGHT_IRIS_RIGHT = 474\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"âŒ Webcam not opened.\")\n",
    "    exit()\n",
    "\n",
    "print(\"ğŸ¥ Tracking started - ESC to exit\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # ì‹œì„  ë¶„ì„\n",
    "    gaze_text = \"\"\n",
    "    face_result = face_mesh.process(rgb)\n",
    "    if face_result.multi_face_landmarks:\n",
    "        face = face_result.multi_face_landmarks[0]\n",
    "        lm = face.landmark\n",
    "\n",
    "        # ì™¼ìª½ ëˆˆ\n",
    "        left_eye_outer = np.array([lm[LEFT_EYE_OUTER].x * w, lm[LEFT_EYE_OUTER].y * h])\n",
    "        left_iris_left = np.array([lm[LEFT_IRIS_LEFT].x * w, lm[LEFT_IRIS_LEFT].y * h])\n",
    "        left_iris_right = np.array([lm[LEFT_IRIS_RIGHT].x * w, lm[LEFT_IRIS_RIGHT].y * h])\n",
    "        left_iris_width = np.linalg.norm(left_iris_right - left_iris_left) + 1e-6\n",
    "        left_ratio = np.linalg.norm(left_iris_left - left_eye_outer) / left_iris_width\n",
    "\n",
    "        # ì˜¤ë¥¸ìª½ ëˆˆ\n",
    "        right_eye_outer = np.array([lm[RIGHT_EYE_OUTER].x * w, lm[RIGHT_EYE_OUTER].y * h])\n",
    "        right_iris_left = np.array([lm[RIGHT_IRIS_LEFT].x * w, lm[RIGHT_IRIS_LEFT].y * h])\n",
    "        right_iris_right = np.array([lm[RIGHT_IRIS_RIGHT].x * w, lm[RIGHT_IRIS_RIGHT].y * h])\n",
    "        right_iris_width = np.linalg.norm(right_iris_right - right_iris_left) + 1e-6\n",
    "        right_ratio = np.linalg.norm(right_eye_outer - right_iris_right) / right_iris_width\n",
    "\n",
    "        # ì‹œì„  ë°©í–¥ ê²°ì •\n",
    "        left_gaze = \"LEFT\" if left_ratio < 0.48 else \"CENTER\"\n",
    "        right_gaze = \"RIGHT\" if right_ratio < 0.48 else \"CENTER\"\n",
    "        result_gaze = \"LEFT\" if left_gaze == \"LEFT\" else \"RIGHT\" if right_gaze == \"RIGHT\" else \"CENTER\"\n",
    "\n",
    "        gaze_text = f\"L-Gaze:{left_gaze}({left_ratio:.2f})|R-Gaze:{right_gaze}({right_ratio:.2f})â†’{result_gaze}\"\n",
    "\n",
    "        # ë””ë²„ê¹…ìš© ëˆˆ ì£¼ë³€ ì \n",
    "        for idx in [LEFT_EYE_OUTER, LEFT_IRIS_LEFT, LEFT_IRIS_RIGHT,\n",
    "                    RIGHT_EYE_OUTER, RIGHT_IRIS_LEFT, RIGHT_IRIS_RIGHT]:\n",
    "            cx, cy = int(lm[idx].x * w), int(lm[idx].y * h)\n",
    "            cv2.circle(frame, (cx, cy), 3, (0, 255, 255), -1)\n",
    "\n",
    "    # í¬ì¦ˆ ë¶„ì„\n",
    "    pose_result = pose.process(rgb)\n",
    "    pose_text = \"\"\n",
    "    if pose_result.pose_landmarks:\n",
    "        landmarks = pose_result.pose_landmarks.landmark\n",
    "\n",
    "        # ê³ ê°œ ë°©í–¥ (ì¢Œìš° ê·€ ê¸°ì¤€)\n",
    "        left_ear_x = landmarks[mp_pose.PoseLandmark.LEFT_EAR].x\n",
    "        right_ear_x = landmarks[mp_pose.PoseLandmark.RIGHT_EAR].x\n",
    "        head_text = \"\"\n",
    "        if left_ear_x - right_ear_x > 0.08:\n",
    "            head_text = \"HEAD â†’ LEFT\"\n",
    "        elif right_ear_x - left_ear_x > 0.08:\n",
    "            head_text = \"HEAD â†’ RIGHT\"\n",
    "        else:\n",
    "            head_text = \"HEAD â†’ CENTER\"\n",
    "\n",
    "        # ì–´ê¹¨ ë†’ì´ ë¹„êµ\n",
    "        left_shoulder_y = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y\n",
    "        right_shoulder_y = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y\n",
    "        diff = left_shoulder_y - right_shoulder_y\n",
    "        if diff > 0.012:\n",
    "            shoulder_text = \"LEFT SHOULDER UP\"\n",
    "        elif diff < -0.012:\n",
    "            shoulder_text = \"RIGHT SHOULDER UP\"\n",
    "        else:\n",
    "            shoulder_text = \"SHOULDERS STRAIGHT\"\n",
    "\n",
    "        pose_text = f\"{head_text} | ğŸ§ {shoulder_text}\"\n",
    "\n",
    "        # í¬ì¦ˆ ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            pose_result.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2)\n",
    "        )\n",
    "\n",
    "    # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶œë ¥\n",
    "    y = 30\n",
    "    for line in [gaze_text, pose_text]:\n",
    "        if line:\n",
    "            cv2.putText(frame, line, (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            y += 30\n",
    "\n",
    "    cv2.imshow(\"Full Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67cff36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ Gaze + Head + Shoulder Tracking Started - ESC to exit\n"
     ]
    }
   ],
   "source": [
    "# ì¢Œìš° ì‹œì„ , ê³ ê°œ, ì–´ê¹¨ ì™„ì„±\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "\n",
    "# FaceMesh & Pose ëœë“œë§ˆí¬ ì¸ë±ìŠ¤\n",
    "LEFT_EYE_OUTER = 33\n",
    "LEFT_IRIS_LEFT = 471\n",
    "LEFT_IRIS_RIGHT = 469\n",
    "LEFT_EYE_CENTER = 468\n",
    "\n",
    "RIGHT_EYE_OUTER = 263\n",
    "RIGHT_IRIS_LEFT = 476\n",
    "RIGHT_IRIS_RIGHT = 474\n",
    "RIGHT_EYE_CENTER = 473\n",
    "\n",
    "LEFT_EAR = mp_pose.PoseLandmark.LEFT_EAR\n",
    "RIGHT_EAR = mp_pose.PoseLandmark.RIGHT_EAR\n",
    "LEFT_SHOULDER = mp_pose.PoseLandmark.LEFT_SHOULDER\n",
    "RIGHT_SHOULDER = mp_pose.PoseLandmark.RIGHT_SHOULDER\n",
    "\n",
    "# ì›¹ìº  ì‹œì‘\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"âŒ Webcam not opened.\")\n",
    "    exit()\n",
    "\n",
    "print(\"ğŸ¥ Gaze + Head + Shoulder Tracking Started - ESC to exit\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # FaceMesh ì‹œì„  ë¶„ì„\n",
    "    gaze_text = \"\"\n",
    "    head_text = \"\"\n",
    "    face_result = face_mesh.process(rgb)\n",
    "\n",
    "    # Pose ë¶„ì„\n",
    "    pose_result = pose.process(rgb)\n",
    "\n",
    "    if face_result.multi_face_landmarks:\n",
    "        face = face_result.multi_face_landmarks[0]\n",
    "        lm = face.landmark\n",
    "\n",
    "        # === ì™¼ìª½ ëˆˆ ì‹œì„  ë¶„ì„ ===\n",
    "        left_eye_outer = np.array([lm[LEFT_EYE_OUTER].x * w, lm[LEFT_EYE_OUTER].y * h])\n",
    "        left_iris_left = np.array([lm[LEFT_IRIS_LEFT].x * w, lm[LEFT_IRIS_LEFT].y * h])\n",
    "        left_iris_right = np.array([lm[LEFT_IRIS_RIGHT].x * w, lm[LEFT_IRIS_RIGHT].y * h])\n",
    "        left_iris_width = np.linalg.norm(left_iris_right - left_iris_left) + 1e-6\n",
    "        left_ratio = np.linalg.norm(left_iris_left - left_eye_outer) / left_iris_width\n",
    "\n",
    "        # === ì˜¤ë¥¸ìª½ ëˆˆ ì‹œì„  ë¶„ì„ ===\n",
    "        right_eye_outer = np.array([lm[RIGHT_EYE_OUTER].x * w, lm[RIGHT_EYE_OUTER].y * h])\n",
    "        right_iris_left = np.array([lm[RIGHT_IRIS_LEFT].x * w, lm[RIGHT_IRIS_LEFT].y * h])\n",
    "        right_iris_right = np.array([lm[RIGHT_IRIS_RIGHT].x * w, lm[RIGHT_IRIS_RIGHT].y * h])\n",
    "        right_iris_width = np.linalg.norm(right_iris_right - right_iris_left) + 1e-6\n",
    "        right_ratio = np.linalg.norm(right_eye_outer - right_iris_right) / right_iris_width\n",
    "\n",
    "        # ì‹œì„  ë°©í–¥ ê²°ì •\n",
    "        left_gaze = \"LEFT\" if left_ratio < 0.48 else \"CENTER\"\n",
    "        right_gaze = \"RIGHT\" if right_ratio < 0.48 else \"CENTER\"\n",
    "        result_gaze = \"LEFT\" if left_gaze == \"LEFT\" else \"RIGHT\" if right_gaze == \"RIGHT\" else \"CENTER\"\n",
    "\n",
    "        gaze_text = f\"L-Gaze:{left_gaze}({left_ratio:.2f})|R-Gaze:{right_gaze}({right_ratio:.2f})â†’{result_gaze}\"\n",
    "\n",
    "        # === ê³ ê°œ ë°©í–¥: ì–‘ ëˆˆ â†” ê·€ ê±°ë¦¬ ë¹„êµ ===\n",
    "        if pose_result.pose_landmarks:\n",
    "            pose_lm = pose_result.pose_landmarks.landmark\n",
    "\n",
    "            left_eye_center = lm[LEFT_EYE_CENTER]\n",
    "            right_eye_center = lm[RIGHT_EYE_CENTER]\n",
    "            left_ear = pose_lm[LEFT_EAR]\n",
    "            right_ear = pose_lm[RIGHT_EAR]\n",
    "\n",
    "            left_eye_to_ear = abs(left_eye_center.x - left_ear.x)\n",
    "            right_eye_to_ear = abs(right_eye_center.x - right_ear.x)\n",
    "\n",
    "            if left_eye_to_ear > right_eye_to_ear + 0.03:\n",
    "                head_text = \"Head: LEFT TURN\"\n",
    "            elif right_eye_to_ear > left_eye_to_ear + 0.03:\n",
    "                head_text = \"Head: RIGHT TURN\"\n",
    "            else:\n",
    "                head_text = \"Head: CENTER\"\n",
    "\n",
    "        # ë””ë²„ê¹…ìš© ëˆˆ/í™ì±„ ì \n",
    "        for idx in [LEFT_EYE_OUTER, LEFT_IRIS_LEFT, LEFT_IRIS_RIGHT,\n",
    "                    RIGHT_EYE_OUTER, RIGHT_IRIS_LEFT, RIGHT_IRIS_RIGHT]:\n",
    "            cx, cy = int(lm[idx].x * w), int(lm[idx].y * h)\n",
    "            cv2.circle(frame, (cx, cy), 3, (0, 255, 255), -1)\n",
    "\n",
    "    # === ì–´ê¹¨ ìì„¸ ===\n",
    "    shoulder_text = \"\"\n",
    "    if pose_result.pose_landmarks:\n",
    "        lm = pose_result.pose_landmarks.landmark\n",
    "        left_shoulder_y = lm[LEFT_SHOULDER].y\n",
    "        right_shoulder_y = lm[RIGHT_SHOULDER].y\n",
    "        diff = left_shoulder_y - right_shoulder_y\n",
    "\n",
    "        if diff > 0.012:\n",
    "            shoulder_text = \"LEFT SHOULDER UP\"\n",
    "        elif diff < -0.012:\n",
    "            shoulder_text = \"RIGHT SHOULDER UP\"\n",
    "        else:\n",
    "            shoulder_text = \"SHOULDERS STRAIGHT\"\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            pose_result.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2)\n",
    "        )\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ì¶œë ¥\n",
    "    y = 30\n",
    "    for text in [gaze_text, head_text, shoulder_text]:\n",
    "        if text:\n",
    "            cv2.putText(frame, text, (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            y += 30\n",
    "\n",
    "    cv2.imshow(\"Full Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93af7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ Tracking Started - ESC to exit\n",
      "ğŸ‘ï¸ ì´ì „ ì‹œì„  'NONE' ìœ ì§€ ì‹œê°„: 0.33s\n"
     ]
    }
   ],
   "source": [
    "# ì¢Œìš° ì™„ì„± + ìœ„, ì•„ë˜ ì¶”ê°€\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "\n",
    "# ëœë“œë§ˆí¬ ì¸ë±ìŠ¤\n",
    "LEFT_EYE_OUTER = 33\n",
    "LEFT_IRIS_LEFT = 471\n",
    "LEFT_IRIS_RIGHT = 469\n",
    "LEFT_IRIS_TOP = 470\n",
    "LEFT_IRIS_BOTTOM = 472\n",
    "LEFT_EYE_CENTER = 468\n",
    "\n",
    "RIGHT_EYE_OUTER = 263\n",
    "RIGHT_IRIS_LEFT = 476\n",
    "RIGHT_IRIS_RIGHT = 474\n",
    "RIGHT_IRIS_TOP = 475\n",
    "RIGHT_IRIS_BOTTOM = 477\n",
    "RIGHT_EYE_CENTER = 473\n",
    "\n",
    "LEFT_EAR = mp_pose.PoseLandmark.LEFT_EAR\n",
    "RIGHT_EAR = mp_pose.PoseLandmark.RIGHT_EAR\n",
    "LEFT_SHOULDER = mp_pose.PoseLandmark.LEFT_SHOULDER\n",
    "RIGHT_SHOULDER = mp_pose.PoseLandmark.RIGHT_SHOULDER\n",
    "\n",
    "# ì›¹ìº \n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"âŒ Webcam not opened.\")\n",
    "    exit()\n",
    "\n",
    "print(\"ğŸ¥ Tracking Started - ESC to exit\")\n",
    "\n",
    "prev_time = time.time()\n",
    "prev_gaze = \"NONE\"\n",
    "gaze_start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    face_result = face_mesh.process(rgb)\n",
    "    pose_result = pose.process(rgb)\n",
    "\n",
    "    gaze_text = \"\"\n",
    "    head_text = \"\"\n",
    "    shoulder_text = \"\"\n",
    "    gaze_direction = \"\"\n",
    "\n",
    "    if face_result.multi_face_landmarks:\n",
    "        face = face_result.multi_face_landmarks[0]\n",
    "        lm = face.landmark\n",
    "\n",
    "        # === ì™¼ìª½ ëˆˆ (ì¢Œìš°/ìƒí•˜) ===\n",
    "        left_eye_outer = np.array([lm[LEFT_EYE_OUTER].x * w, lm[LEFT_EYE_OUTER].y * h])\n",
    "        left_iris_left = np.array([lm[LEFT_IRIS_LEFT].x * w, lm[LEFT_IRIS_LEFT].y * h])\n",
    "        left_iris_right = np.array([lm[LEFT_IRIS_RIGHT].x * w, lm[LEFT_IRIS_RIGHT].y * h])\n",
    "        left_iris_top = np.array([lm[LEFT_IRIS_TOP].x * w, lm[LEFT_IRIS_TOP].y * h])\n",
    "        left_iris_bottom = np.array([lm[LEFT_IRIS_BOTTOM].x * w, lm[LEFT_IRIS_BOTTOM].y * h])\n",
    "\n",
    "        left_iris_width = np.linalg.norm(left_iris_right - left_iris_left) + 1e-6\n",
    "        left_iris_height = np.linalg.norm(left_iris_top - left_iris_bottom) + 1e-6\n",
    "\n",
    "        left_ratio_h = np.linalg.norm(left_iris_left - left_eye_outer) / left_iris_width\n",
    "        left_ratio_v = np.linalg.norm(left_iris_top - left_iris_bottom) / left_iris_height\n",
    "\n",
    "        # === ì˜¤ë¥¸ìª½ ëˆˆ (ì¢Œìš°/ìƒí•˜) ===\n",
    "        right_eye_outer = np.array([lm[RIGHT_EYE_OUTER].x * w, lm[RIGHT_EYE_OUTER].y * h])\n",
    "        right_iris_left = np.array([lm[RIGHT_IRIS_LEFT].x * w, lm[RIGHT_IRIS_LEFT].y * h])\n",
    "        right_iris_right = np.array([lm[RIGHT_IRIS_RIGHT].x * w, lm[RIGHT_IRIS_RIGHT].y * h])\n",
    "        right_iris_top = np.array([lm[RIGHT_IRIS_TOP].x * w, lm[RIGHT_IRIS_TOP].y * h])\n",
    "        right_iris_bottom = np.array([lm[RIGHT_IRIS_BOTTOM].x * w, lm[RIGHT_IRIS_BOTTOM].y * h])\n",
    "\n",
    "        right_iris_width = np.linalg.norm(right_iris_right - right_iris_left) + 1e-6\n",
    "        right_iris_height = np.linalg.norm(right_iris_top - right_iris_bottom) + 1e-6\n",
    "\n",
    "        right_ratio_h = np.linalg.norm(right_iris_right - right_eye_outer) / right_iris_width\n",
    "        right_ratio_v = np.linalg.norm(right_iris_top - right_iris_bottom) / right_iris_height\n",
    "\n",
    "        # ì‹œì„  ë°©í–¥ ê²°ì •\n",
    "        gaze_h = \"LEFT\" if left_ratio_h < 0.48 else \"RIGHT\" if right_ratio_h < 0.48 else \"CENTER\"\n",
    "        gaze_v = \"UP\" if left_ratio_v > 0.55 else \"DOWN\" if left_ratio_v < 0.45 else \"CENTER\"\n",
    "        gaze_direction = f\"{gaze_h} / {gaze_v}\"\n",
    "\n",
    "        # ì‹œì„  ìœ ì§€ ì‹œê°„ ê³„ì‚°\n",
    "        now = time.time()\n",
    "        if gaze_direction != prev_gaze:\n",
    "            duration = now - gaze_start_time\n",
    "            print(f\"ğŸ‘ï¸ ì´ì „ ì‹œì„  '{prev_gaze}' ìœ ì§€ ì‹œê°„: {duration:.2f}s\")\n",
    "            gaze_start_time = now\n",
    "            prev_gaze = gaze_direction\n",
    "\n",
    "        gaze_text = f\"Gaze: {gaze_direction}\"\n",
    "\n",
    "        # === ê³ ê°œ ë°©í–¥ ===\n",
    "        if pose_result.pose_landmarks:\n",
    "            pose_lm = pose_result.pose_landmarks.landmark\n",
    "            left_eye_center = lm[LEFT_EYE_CENTER]\n",
    "            right_eye_center = lm[RIGHT_EYE_CENTER]\n",
    "            left_ear = pose_lm[LEFT_EAR]\n",
    "            right_ear = pose_lm[RIGHT_EAR]\n",
    "\n",
    "            # ì¢Œìš°\n",
    "            left_eye_to_ear = abs(left_eye_center.x - left_ear.x)\n",
    "            right_eye_to_ear = abs(right_eye_center.x - right_ear.x)\n",
    "            if left_eye_to_ear > right_eye_to_ear + 0.03:\n",
    "                head_lr = \"LEFT\"\n",
    "            elif right_eye_to_ear > left_eye_to_ear + 0.03:\n",
    "                head_lr = \"RIGHT\"\n",
    "            else:\n",
    "                head_lr = \"CENTER\"\n",
    "\n",
    "            # ìƒí•˜ (ëˆˆ â†’ ì–´ê¹¨ ì¤‘ê°„ì  ë†’ì´ ë¹„êµ)\n",
    "            nose_y = (left_eye_center.y + right_eye_center.y) / 2\n",
    "            shoulder_y = (pose_lm[LEFT_SHOULDER].y + pose_lm[RIGHT_SHOULDER].y) / 2\n",
    "            head_ud = str(nose_y) + str(shoulder_y)\n",
    "\n",
    "            # if nose_y < shoulder_y - 0.05:\n",
    "            #     head_ud = \"UP\"\n",
    "            # elif nose_y > shoulder_y + 0.05:\n",
    "            #     head_ud = \"DOWN\"\n",
    "            # else:\n",
    "            #     head_ud = \"CENTER\"\n",
    "\n",
    "            head_text = f\"Head: {head_lr} / {head_ud}\"\n",
    "\n",
    "    # === ì–´ê¹¨ ê¸°ìš¸ê¸° ===\n",
    "    if pose_result.pose_landmarks:\n",
    "        lm = pose_result.pose_landmarks.landmark\n",
    "        diff = lm[LEFT_SHOULDER].y - lm[RIGHT_SHOULDER].y\n",
    "        if diff > 0.012:\n",
    "            shoulder_text = \"LEFT SHOULDER UP\"\n",
    "        elif diff < -0.012:\n",
    "            shoulder_text = \"RIGHT SHOULDER UP\"\n",
    "        else:\n",
    "            shoulder_text = \"SHOULDERS STRAIGHT\"\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            pose_result.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2)\n",
    "        )\n",
    "\n",
    "    # ì¶œë ¥\n",
    "    y = 30\n",
    "    for text in [gaze_text, head_text, shoulder_text]:\n",
    "        if text:\n",
    "            cv2.putText(frame, text, (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            y += 30\n",
    "\n",
    "    cv2.imshow(\"Gaze + Head + Shoulder (Full)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0245fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì„ , ê³ ê°œë§Œ ìœ„ì•„ë˜ - chat gpt ìˆ˜ì¹˜ ê·¸ëŒ€ë¡œ - ê³ ê°œ ì—„ì²­ ìœ„ë¡œ ë“¤ì–´ì•¼ center.  ì‹œì„ ì€ ì•„ì˜ˆ ì•ˆë¨.\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# ì›¹ìº  ì—´ê¸°\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # ì‹œì„  íŒë‹¨ ê¸°ì¤€ ëœë“œë§ˆí¬ (ì™¼ìª½ ëˆˆ ê¸°ì¤€)\n",
    "            iris_y = face_landmarks.landmark[468].y\n",
    "            eye_top_y = face_landmarks.landmark[159].y\n",
    "            eye_bottom_y = face_landmarks.landmark[145].y\n",
    "\n",
    "            # ì‹œì„  ë°©í–¥ íŒë³„\n",
    "            if iris_y < eye_top_y - 0.005:\n",
    "                gaze_direction = \"ğŸ‘ï¸ UP\"\n",
    "            elif iris_y > eye_bottom_y + 0.005:\n",
    "                gaze_direction = \"ğŸ‘ï¸ DOWN\"\n",
    "            else:\n",
    "                gaze_direction = \"ğŸ‘ï¸ CENTER\"\n",
    "\n",
    "            # ê³ ê°œ ë°©í–¥ íŒë‹¨ ê¸°ì¤€ (ëˆˆ, ì½”ë, í„±)\n",
    "            left_eye_y = (face_landmarks.landmark[33].y + face_landmarks.landmark[133].y) / 2\n",
    "            nose_y = face_landmarks.landmark[1].y\n",
    "            chin_y = face_landmarks.landmark[152].y\n",
    "\n",
    "            # ê³ ê°œ ë°©í–¥ íŒë³„\n",
    "            if nose_y < left_eye_y - 0.02:\n",
    "                head_direction = \"ğŸ§‘â€ğŸ¦° HEAD UP\"\n",
    "            elif nose_y > left_eye_y + 0.02:\n",
    "                head_direction = \"ğŸ§‘â€ğŸ¦° HEAD DOWN\"\n",
    "            else:\n",
    "                head_direction = \"ğŸ§‘â€ğŸ¦° HEAD CENTER\"\n",
    "\n",
    "            # ì‹œê°í™”\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                face_landmarks,\n",
    "                mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1)\n",
    "            )\n",
    "\n",
    "            cv2.putText(frame, gaze_direction, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "            cv2.putText(frame, head_direction, (30, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 128, 0), 2)\n",
    "\n",
    "    cv2.imshow('Gaze & Head Direction (Vertical)', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5999424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì„ , ê³ ê°œë§Œ ìœ„ì•„ë˜ - chat gpt ìˆ˜ì¹˜ë§Œ ìˆ˜ì • - ì˜ ì•ˆë¨\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# ì›¹ìº  ì—´ê¸°\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # ì‹œì„  íŒë‹¨ ê¸°ì¤€ ëœë“œë§ˆí¬ (ì™¼ìª½ ëˆˆ ê¸°ì¤€)\n",
    "            iris_y = face_landmarks.landmark[468].y\n",
    "            eye_top_y = face_landmarks.landmark[159].y\n",
    "            eye_bottom_y = face_landmarks.landmark[145].y\n",
    "\n",
    "            # ì‹œì„  ë°©í–¥ íŒë³„\n",
    "            if iris_y < eye_top_y - 0.005:\n",
    "                gaze_direction = \"UP\"\n",
    "            elif iris_y > eye_bottom_y + 0.005:\n",
    "                gaze_direction = \"DOWN\"\n",
    "            else:\n",
    "                gaze_direction = \"CENTER\"\n",
    "\n",
    "            # ê³ ê°œ ë°©í–¥ íŒë‹¨ ê¸°ì¤€ (ëˆˆ, ì½”ë, í„±)\n",
    "            left_eye_y = (face_landmarks.landmark[33].y + face_landmarks.landmark[133].y) / 2\n",
    "            nose_y = face_landmarks.landmark[1].y\n",
    "            chin_y = face_landmarks.landmark[152].y\n",
    "\n",
    "            # ê³ ê°œ ë°©í–¥ íŒë³„\n",
    "            if nose_y < left_eye_y - 0.3:   # 0.4 < 0.8 -0.3 (0.5)\n",
    "                head_direction = \"HEAD UP\"\n",
    "            elif nose_y > left_eye_y - 0.5:   # 0.4 > 0.8 -0.5 (0.3)\n",
    "                head_direction = \"HEAD DOWN\"\n",
    "            else:\n",
    "                head_direction = \"HEAD CENTER\"\n",
    "\n",
    "            # ì‹œê°í™”\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                face_landmarks,\n",
    "                mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1)\n",
    "            )\n",
    "\n",
    "            cv2.putText(frame, gaze_direction, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "            cv2.putText(frame, head_direction, (30, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 128, 0), 2)\n",
    "\n",
    "    cv2.imshow('Gaze & Head Direction (Vertical)', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3908b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›¹ìº  ì‹œì„  ê³ ê°œ íŒë‹¨ ì±„íŒ…ì°½ -> MediaPipe + solvePnP ì¡°í•©ìœ¼ë¡œ ê³ ê°œ & ì‹œì„  ìœ„/ì•„ë˜ ì‹¤ì‹œê°„ íŒë‹¨í•˜ëŠ” ì™„ì„± ì½”ë“œ ì‘ì„±í•´ì¤˜\n",
    "# ìœ„ì•„ë˜ ê³ ê°œëŠ” ì˜ë¨ -> but ìœ„ë¡œëŠ” ì¡°ê¸ˆ ë” ë“¤ì–´ì•¼ up, ì•„ë˜ë¡œëŠ” ëœ ë‚´ë ¤ë„ down ë˜ê²Œ. pitch > -15,   pitch > 5\n",
    "# ì‹œì„ ì€ ì˜ ì•ˆë¨. -> êµ‰ì¥íˆ ìœ„ë¥¼ ë°”ë¼ë³¼ ë•Œë§Œ, ìœ„ê°€ ëœ¸. ì•„ë˜ëŠ” ì˜ ì•ˆ ë¨.\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from math import degrees\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "# ì–¼êµ´ì˜ 3D ëª¨ë¸ ì¢Œí‘œ (solvePnPìš©)\n",
    "model_points = np.array([\n",
    "    (0.0, 0.0, 0.0),          # Nose tip - landmark 1\n",
    "    (0.0, -63.6, -12.5),      # Chin - landmark 152\n",
    "    (-43.3, 32.7, -26.0),     # Left eye left corner - landmark 33\n",
    "    (43.3, 32.7, -26.0),      # Right eye right corner - landmark 263\n",
    "    (-28.9, -28.9, -24.1),    # Left Mouth corner - landmark 78\n",
    "    (28.9, -28.9, -24.1)      # Right Mouth corner - landmark 308\n",
    "], dtype=np.float32)\n",
    "\n",
    "# ì¹´ë©”ë¼ ì„¤ì • (ì„ì˜ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°)\n",
    "def get_camera_matrix(w, h):\n",
    "    focal_length = w\n",
    "    center = (w / 2, h / 2)\n",
    "    return np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "# ì›¹ìº  ì—´ê¸°\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w = frame.shape[:2]\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "        # 2D ì´ë¯¸ì§€ ì¢Œí‘œ: solvePnPìš©\n",
    "        image_points = np.array([\n",
    "            [face_landmarks[1].x * w, face_landmarks[1].y * h],     # Nose tip\n",
    "            [face_landmarks[152].x * w, face_landmarks[152].y * h], # Chin\n",
    "            [face_landmarks[33].x * w, face_landmarks[33].y * h],   # Left eye left corner\n",
    "            [face_landmarks[263].x * w, face_landmarks[263].y * h], # Right eye right corner\n",
    "            [face_landmarks[78].x * w, face_landmarks[78].y * h],   # Left mouth corner\n",
    "            [face_landmarks[308].x * w, face_landmarks[308].y * h]  # Right mouth corner\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        camera_matrix = get_camera_matrix(w, h)\n",
    "        dist_coeffs = np.zeros((4, 1))  # ì™œê³¡ ë¬´ì‹œ\n",
    "\n",
    "        # solvePnP â†’ ê³ ê°œ íšŒì „ ë²¡í„°\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            model_points, image_points, camera_matrix, dist_coeffs\n",
    "        )\n",
    "\n",
    "        # íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜\n",
    "        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "        pitch = degrees(np.arcsin(-rotation_matrix[2][1]))\n",
    "\n",
    "        # ê³ ê°œ ìœ„/ì•„ë˜ íŒë‹¨\n",
    "        if pitch > 5:\n",
    "            head_direction = \"Head Down\"\n",
    "        elif pitch < -15:\n",
    "            head_direction = \"Head Up\"\n",
    "        else:\n",
    "            head_direction = \"Head Center\"\n",
    "\n",
    "        # ì‹œì„  ìœ„/ì•„ë˜ íŒë‹¨ (ì™¼ìª½ ëˆˆ ê¸°ì¤€)\n",
    "        iris_y = face_landmarks[468].y\n",
    "        eye_top_y = face_landmarks[159].y\n",
    "        eye_bottom_y = face_landmarks[145].y\n",
    "        iris_ratio = (iris_y - eye_top_y) / (eye_bottom_y - eye_top_y)\n",
    "\n",
    "        if iris_ratio < 0.4:\n",
    "            gaze_direction = \"Gaze Up\"\n",
    "        elif iris_ratio > 0.6:\n",
    "            gaze_direction = \"Gaze Down\"\n",
    "        else:\n",
    "            gaze_direction = \"Gaze Center\"\n",
    "\n",
    "        # ì‹œê°í™”\n",
    "        cv2.putText(frame, f\"{head_direction}\", (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 128, 0), 2)\n",
    "        cv2.putText(frame, f\"{gaze_direction}\", (30, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('Head & Gaze Pitch Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "addf1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›¹ìº  ì‹œì„  ê³ ê°œ íŒë‹¨ ì±„íŒ…ì°½ -> MediaPipe + solvePnP ì¡°í•©ìœ¼ë¡œ ê³ ê°œ & ì‹œì„  ìœ„/ì•„ë˜ ì‹¤ì‹œê°„ íŒë‹¨í•˜ëŠ” ì™„ì„± ì½”ë“œ ì‘ì„±í•´ì¤˜\n",
    "# ìœ„ì•„ë˜ ê³ ê°œëŠ” ì˜ë¨ -> ìˆ˜ì¹˜ ë²”ìœ„ -> ìˆ˜ì •\n",
    "# ì‹œì„ ì€ ì˜ ì•ˆë¨. -> êµ‰ì¥íˆ ìœ„ë¥¼ ë°”ë¼ë³¼ ë•Œë§Œ, ìœ„ê°€ ëœ¸. ì•„ë˜ëŠ” ì˜ ì•ˆ ë¨.\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from math import degrees\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "# ì–¼êµ´ì˜ 3D ëª¨ë¸ ì¢Œí‘œ (solvePnPìš©)\n",
    "model_points = np.array([\n",
    "    (0.0, 0.0, 0.0),          # Nose tip - landmark 1\n",
    "    (0.0, -63.6, -12.5),      # Chin - landmark 152\n",
    "    (-43.3, 32.7, -26.0),     # Left eye left corner - landmark 33\n",
    "    (43.3, 32.7, -26.0),      # Right eye right corner - landmark 263\n",
    "    (-28.9, -28.9, -24.1),    # Left Mouth corner - landmark 78\n",
    "    (28.9, -28.9, -24.1)      # Right Mouth corner - landmark 308\n",
    "], dtype=np.float32)\n",
    "\n",
    "# ì¹´ë©”ë¼ ì„¤ì • (ì„ì˜ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°)\n",
    "def get_camera_matrix(w, h):\n",
    "    focal_length = w\n",
    "    center = (w / 2, h / 2)\n",
    "    return np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "# ì›¹ìº  ì—´ê¸°\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w = frame.shape[:2]\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "        # 2D ì´ë¯¸ì§€ ì¢Œí‘œ: solvePnPìš©\n",
    "        image_points = np.array([\n",
    "            [face_landmarks[1].x * w, face_landmarks[1].y * h],     # Nose tip\n",
    "            [face_landmarks[152].x * w, face_landmarks[152].y * h], # Chin\n",
    "            [face_landmarks[33].x * w, face_landmarks[33].y * h],   # Left eye left corner\n",
    "            [face_landmarks[263].x * w, face_landmarks[263].y * h], # Right eye right corner\n",
    "            [face_landmarks[78].x * w, face_landmarks[78].y * h],   # Left mouth corner\n",
    "            [face_landmarks[308].x * w, face_landmarks[308].y * h]  # Right mouth corner\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        camera_matrix = get_camera_matrix(w, h)\n",
    "        dist_coeffs = np.zeros((4, 1))  # ì™œê³¡ ë¬´ì‹œ\n",
    "\n",
    "        # solvePnP â†’ ê³ ê°œ íšŒì „ ë²¡í„°\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            model_points, image_points, camera_matrix, dist_coeffs\n",
    "        )\n",
    "\n",
    "        # íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜\n",
    "        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "        pitch = degrees(np.arcsin(-rotation_matrix[2][1]))\n",
    "\n",
    "        # ê³ ê°œ ìœ„/ì•„ë˜ íŒë‹¨\n",
    "        if pitch > 15:      # 5 -> 15  (ë” ê³ ê°œ ë“¤ì–´ì•¼ ë˜ê²Œ)\n",
    "            head_direction = \"Head Down\"\n",
    "        elif pitch < -2:    # -15 -> -2   (ëœ ê³ ê°œ ìˆ™ì´ì§€ ì•Šì•„ë„ ë˜ê²Œ)\n",
    "            head_direction = \"Head Up\"\n",
    "        else:\n",
    "            head_direction = \"Head Center\"\n",
    "\n",
    "        # ì‹œì„  ìœ„/ì•„ë˜ íŒë‹¨ (ì™¼ìª½ ëˆˆ ê¸°ì¤€)\n",
    "        iris_y = face_landmarks[468].y\n",
    "        eye_top_y = face_landmarks[159].y\n",
    "        eye_bottom_y = face_landmarks[145].y\n",
    "        iris_ratio = (iris_y - eye_top_y) / (eye_bottom_y - eye_top_y)\n",
    "\n",
    "        if iris_ratio < 0.4:\n",
    "            gaze_direction = \"Gaze Up\"\n",
    "        elif iris_ratio > 0.6:\n",
    "            gaze_direction = \"Gaze Down\"\n",
    "        else:\n",
    "            gaze_direction = \"Gaze Center\"\n",
    "\n",
    "        # ì‹œê°í™”\n",
    "        cv2.putText(frame, f\"{head_direction}\", (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 128, 0), 2)\n",
    "        cv2.putText(frame, f\"{gaze_direction}\", (30, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('Head & Gaze Pitch Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8ad8cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gaze_tracking'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtime\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnumpy\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mgaze_tracking\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m GazeTracking\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmediapipe\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# GazeTracking ì´ˆê¸°í™”\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gaze_tracking'"
     ]
    }
   ],
   "source": [
    "# ì‹œì„ ë•Œë¬¸ì— gaze tracking í•´ë³´ë ¤ê³  í–ˆëŠ”ë°, ì•ˆë¼ -> ì„¤ì¹˜ ì–´ë ¤ì›Œì„œ ì‹¤íŒ¨ 0.0.8 ë°›ê¸°ê°€ í˜ë“¬.\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from gaze_tracking import GazeTracking\n",
    "import mediapipe as mp\n",
    "\n",
    "# GazeTracking ì´ˆê¸°í™”\n",
    "gaze = GazeTracking()\n",
    "\n",
    "# MediaPipe FaceMesh ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "# 3D ëª¨ë¸ í¬ì¸íŠ¸ (solvePnPìš©)\n",
    "model_points = np.array([\n",
    "    (0.0, 0.0, 0.0),          # Nose tip\n",
    "    (0.0, -63.6, -12.5),      # Chin\n",
    "    (-43.3, 32.7, -26.0),     # Left eye corner\n",
    "    (43.3, 32.7, -26.0),      # Right eye corner\n",
    "    (-28.9, -28.9, -24.1),    # Left mouth\n",
    "    (28.9, -28.9, -24.1)      # Right mouth\n",
    "], dtype=np.float32)\n",
    "\n",
    "# ì‹œì„  ìœ ì§€ ì‹œê°„ ì¸¡ì •ìš© ë³€ìˆ˜\n",
    "gaze_state = None\n",
    "gaze_start_time = time.time()\n",
    "gaze_duration_dict = {\"UP\": 0.0, \"DOWN\": 0.0, \"CENTER\": 0.0}\n",
    "\n",
    "# ì¹´ë©”ë¼ ì—´ê¸°\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def get_camera_matrix(w, h):\n",
    "    focal_length = w\n",
    "    center = (w / 2, h / 2)\n",
    "    return np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w = frame.shape[:2]\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # GazeTracking ë¶„ì„\n",
    "    gaze.refresh(frame)\n",
    "    vertical_ratio = gaze.vertical_ratio()\n",
    "    gaze_text = \"Gaze: Unknown\"\n",
    "\n",
    "    if vertical_ratio is not None:\n",
    "        if vertical_ratio < 0.35:\n",
    "            gaze_direction = \"UP\"\n",
    "        elif vertical_ratio > 0.65:\n",
    "            gaze_direction = \"DOWN\"\n",
    "        else:\n",
    "            gaze_direction = \"CENTER\"\n",
    "\n",
    "        gaze_text = f\"Gaze: {gaze_direction}\"\n",
    "\n",
    "        # ì‹œì„  ìœ ì§€ ì‹œê°„ ê¸°ë¡\n",
    "        now = time.time()\n",
    "        if gaze_direction != gaze_state:\n",
    "            if gaze_state:\n",
    "                gaze_duration_dict[gaze_state] += now - gaze_start_time\n",
    "            gaze_start_time = now\n",
    "            gaze_state = gaze_direction\n",
    "\n",
    "    # MediaPipeë¡œ ê³ ê°œ ë°©í–¥ (pitch) ê³„ì‚°\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    pitch_text = \"Head: Unknown\"\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "        image_points = np.array([\n",
    "            [face_landmarks[1].x * w, face_landmarks[1].y * h],\n",
    "            [face_landmarks[152].x * w, face_landmarks[152].y * h],\n",
    "            [face_landmarks[33].x * w, face_landmarks[33].y * h],\n",
    "            [face_landmarks[263].x * w, face_landmarks[263].y * h],\n",
    "            [face_landmarks[78].x * w, face_landmarks[78].y * h],\n",
    "            [face_landmarks[308].x * w, face_landmarks[308].y * h]\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        camera_matrix = get_camera_matrix(w, h)\n",
    "        dist_coeffs = np.zeros((4, 1))\n",
    "\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            model_points, image_points, camera_matrix, dist_coeffs\n",
    "        )\n",
    "\n",
    "        if success:\n",
    "            rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "            pitch = np.degrees(np.arcsin(-rotation_matrix[2][1]))\n",
    "\n",
    "            if pitch < 15:\n",
    "                pitch_text = \"Head: UP\"\n",
    "            elif pitch > -2:\n",
    "                pitch_text = \"Head: DOWN\"\n",
    "            else:\n",
    "                pitch_text = \"Head: CENTER\"\n",
    "\n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    display_text = f\"{gaze_text} | {pitch_text}\"\n",
    "    cv2.putText(frame, display_text, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    for i, (k, v) in enumerate(gaze_duration_dict.items()):\n",
    "        cv2.putText(frame, f\"{k}: {v:.1f}s\", (30, 100 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Gaze & Head Pitch Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# ë§ˆì§€ë§‰ ì‹œì„  ëˆ„ì \n",
    "if gaze_state:\n",
    "    gaze_duration_dict[gaze_state] += time.time() - gaze_start_time\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\nì´ ì‹œì„  ìœ ì§€ ì‹œê°„:\")\n",
    "for direction, duration in gaze_duration_dict.items():\n",
    "    print(f\"{direction}: {duration:.2f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e92c0e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gaze-tracking\n",
      "  Downloading gaze_tracking-0.0.1-py3-none-any.whl.metadata (595 bytes)\n",
      "Downloading gaze_tracking-0.0.1-py3-none-any.whl (1.3 kB)\n",
      "Installing collected packages: gaze-tracking\n",
      "Successfully installed gaze-tracking-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gaze-tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ea7d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì´ ì‹œì„  ìœ ì§€ ì‹œê°„:\n",
      "UP: 23.56ì´ˆ\n",
      "DOWN: 0.00ì´ˆ\n",
      "CENTER: 30.16ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "# gaze tracking ì—†ì•°. ì‹œì„  ìœ„ì•„ë˜ëŠ” mediapipeë¡œ ì§„í–‰í•´ì•¼í•  ë“¯. -> ìˆ˜ì¹˜ ì¡°ì •ì¤‘\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "# 3D ëª¨ë¸ í¬ì¸íŠ¸ (solvePnPìš©)\n",
    "model_points = np.array([\n",
    "    (0.0, 0.0, 0.0),\n",
    "    (0.0, -63.6, -12.5),\n",
    "    (-43.3, 32.7, -26.0),\n",
    "    (43.3, 32.7, -26.0),\n",
    "    (-28.9, -28.9, -24.1),\n",
    "    (28.9, -28.9, -24.1)\n",
    "], dtype=np.float32)\n",
    "\n",
    "# ì‹œì„  ìœ ì§€ ì‹œê°„ ì¸¡ì •ìš©\n",
    "gaze_state = None\n",
    "gaze_start_time = time.time()\n",
    "gaze_duration_dict = {\"UP\": 0.0, \"DOWN\": 0.0, \"CENTER\": 0.0}\n",
    "\n",
    "# ì¹´ë©”ë¼ ì„¤ì •\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def get_camera_matrix(w, h):\n",
    "    focal_length = w\n",
    "    center = (w / 2, h / 2)\n",
    "    return np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w = frame.shape[:2]\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    gaze_text = \"Gaze: Unknown\"\n",
    "    pitch_text = \"Head: Unknown\"\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "        # --- ì‹œì„  ë¶„ì„ ---\n",
    "        left_ratio = (landmarks[468].y - landmarks[159].y) / (landmarks[145].y - landmarks[159].y)\n",
    "        right_ratio = (landmarks[473].y - landmarks[386].y) / (landmarks[374].y - landmarks[386].y)\n",
    "        iris_ratio = (left_ratio + right_ratio) / 2\n",
    "\n",
    "        if iris_ratio < 0.40:\n",
    "            gaze_direction = \"UP\"\n",
    "        elif iris_ratio > 0.60:\n",
    "            gaze_direction = \"DOWN\"\n",
    "        else:\n",
    "            gaze_direction = \"CENTER\"\n",
    "\n",
    "        gaze_text = f\"Gaze: {gaze_direction}\"\n",
    "\n",
    "        # ì‹œì„  ìœ ì§€ ì‹œê°„ ì¸¡ì •\n",
    "        now = time.time()\n",
    "        if gaze_direction != gaze_state:\n",
    "            if gaze_state:\n",
    "                gaze_duration_dict[gaze_state] += now - gaze_start_time\n",
    "            gaze_start_time = now\n",
    "            gaze_state = gaze_direction\n",
    "\n",
    "        # --- ê³ ê°œ ë°©í–¥(pitch) ë¶„ì„ ---\n",
    "        image_points = np.array([\n",
    "            [landmarks[1].x * w, landmarks[1].y * h],\n",
    "            [landmarks[152].x * w, landmarks[152].y * h],\n",
    "            [landmarks[33].x * w, landmarks[33].y * h],\n",
    "            [landmarks[263].x * w, landmarks[263].y * h],\n",
    "            [landmarks[78].x * w, landmarks[78].y * h],\n",
    "            [landmarks[308].x * w, landmarks[308].y * h]\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        camera_matrix = get_camera_matrix(w, h)\n",
    "        dist_coeffs = np.zeros((4, 1))\n",
    "\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            model_points, image_points, camera_matrix, dist_coeffs\n",
    "        )\n",
    "\n",
    "        if success:\n",
    "            rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "            pitch = np.degrees(np.arcsin(-rotation_matrix[2][1]))\n",
    "\n",
    "            if pitch < -15:\n",
    "                pitch_text = \"Head: UP\"\n",
    "            elif pitch > 5:\n",
    "                pitch_text = \"Head: DOWN\"\n",
    "            else:\n",
    "                pitch_text = \"Head: CENTER\"\n",
    "\n",
    "    # --- ê²°ê³¼ ì¶œë ¥ ---\n",
    "    display_text = f\"{gaze_text} | {pitch_text}\"\n",
    "    cv2.putText(frame, display_text, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    for i, (k, v) in enumerate(gaze_duration_dict.items()):\n",
    "        cv2.putText(frame, f\"{k}: {v:.1f}s\", (30, 100 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"MediaPipe Gaze & Head Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# ë§ˆì§€ë§‰ ì‹œì„  ëˆ„ì \n",
    "if gaze_state:\n",
    "    gaze_duration_dict[gaze_state] += time.time() - gaze_start_time\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\nì´ ì‹œì„  ìœ ì§€ ì‹œê°„:\")\n",
    "for direction, duration in gaze_duration_dict.items():\n",
    "    print(f\"{direction}: {duration:.2f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12dc63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Calibration] iris_avg=0.401, up<0.321, down>0.481\n",
      "\n",
      "ì´ ì‹œì„  ìœ ì§€ ì‹œê°„:\n",
      "UP: 29.81ì´ˆ\n",
      "DOWN: 66.86ì´ˆ\n",
      "CENTER: 455.62ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "# ì´ˆë°˜ 30í”„ë ˆì„ ì°ì–´ì„œ í™ì±„ ìœ„ì¹˜ ë¹„ìœ¨ í‰ê· (avg_ratio) ì¸¡ì • í›„ ì‹œì„  ê°ì§€ -> ì˜ ì•ˆë¨ ì¡°ê¸ˆ ë‚˜ì•„ì§\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "# 3D ëª¨ë¸ í¬ì¸íŠ¸ (solvePnPìš©)\n",
    "model_points = np.array([\n",
    "    (0.0, 0.0, 0.0),\n",
    "    (0.0, -63.6, -12.5),\n",
    "    (-43.3, 32.7, -26.0),\n",
    "    (43.3, 32.7, -26.0),\n",
    "    (-28.9, -28.9, -24.1),\n",
    "    (28.9, -28.9, -24.1)\n",
    "], dtype=np.float32)\n",
    "\n",
    "# ì‹œì„  ìœ ì§€ ì‹œê°„ ì¸¡ì •ìš©\n",
    "gaze_state = None\n",
    "gaze_start_time = time.time()\n",
    "gaze_duration_dict = {\"UP\": 0.0, \"DOWN\": 0.0, \"CENTER\": 0.0}\n",
    "\n",
    "# ì¹´ë©”ë¼ ì„¤ì •\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def get_camera_matrix(w, h):\n",
    "    focal_length = w\n",
    "    center = (w / 2, h / 2)\n",
    "    return np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "# --- ì‹œì„  ë¹„ìœ¨ ìë™ ë³´ì •ìš© ---\n",
    "calibration_ratios = []\n",
    "frame_idx = 0\n",
    "is_calibrated = False\n",
    "up_thresh = 0.0\n",
    "down_thresh = 1.0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w = frame.shape[:2]\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    gaze_text = \"Gaze: Unknown\"\n",
    "    pitch_text = \"Head: Unknown\"\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "        # --- ì‹œì„  ë¶„ì„ ---\n",
    "        left_ratio = (landmarks[468].y - landmarks[159].y) / (landmarks[145].y - landmarks[159].y)\n",
    "        right_ratio = (landmarks[473].y - landmarks[386].y) / (landmarks[374].y - landmarks[386].y)\n",
    "        iris_ratio = (left_ratio + right_ratio) / 2\n",
    "\n",
    "        # ìë™ ë³´ì • ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ì´ˆê¸° 30í”„ë ˆì„ í‰ê· )\n",
    "        if not is_calibrated:\n",
    "            calibration_ratios.append(iris_ratio)\n",
    "            frame_idx += 1\n",
    "            if frame_idx == 30:\n",
    "                avg_ratio = np.mean(calibration_ratios)\n",
    "                up_thresh = avg_ratio - 0.08\n",
    "                down_thresh = avg_ratio + 0.08\n",
    "                is_calibrated = True\n",
    "                print(f\"[Calibration] iris_avg={avg_ratio:.3f}, up<{up_thresh:.3f}, down>{down_thresh:.3f}\")\n",
    "            gaze_direction = \"CENTER\"\n",
    "        else:\n",
    "            if iris_ratio < up_thresh:\n",
    "                gaze_direction = \"UP\"\n",
    "            elif iris_ratio > down_thresh:\n",
    "                gaze_direction = \"DOWN\"\n",
    "            else:\n",
    "                gaze_direction = \"CENTER\"\n",
    "\n",
    "        gaze_text = f\"Gaze: {gaze_direction}\"\n",
    "\n",
    "        # ì‹œì„  ìœ ì§€ ì‹œê°„ ì¸¡ì •\n",
    "        now = time.time()\n",
    "        if gaze_direction != gaze_state:\n",
    "            if gaze_state:\n",
    "                gaze_duration_dict[gaze_state] += now - gaze_start_time\n",
    "            gaze_start_time = now\n",
    "            gaze_state = gaze_direction\n",
    "\n",
    "        # --- ê³ ê°œ ë°©í–¥(pitch) ë¶„ì„ ---\n",
    "        image_points = np.array([\n",
    "            [landmarks[1].x * w, landmarks[1].y * h],\n",
    "            [landmarks[152].x * w, landmarks[152].y * h],\n",
    "            [landmarks[33].x * w, landmarks[33].y * h],\n",
    "            [landmarks[263].x * w, landmarks[263].y * h],\n",
    "            [landmarks[78].x * w, landmarks[78].y * h],\n",
    "            [landmarks[308].x * w, landmarks[308].y * h]\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        camera_matrix = get_camera_matrix(w, h)\n",
    "        dist_coeffs = np.zeros((4, 1))\n",
    "\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            model_points, image_points, camera_matrix, dist_coeffs\n",
    "        )\n",
    "\n",
    "        if success:\n",
    "            rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "            pitch = np.degrees(np.arcsin(-rotation_matrix[2][1]))\n",
    "\n",
    "            if pitch < -15:\n",
    "                pitch_text = \"Head: UP\"\n",
    "            elif pitch > 5:\n",
    "                pitch_text = \"Head: DOWN\"\n",
    "            else:\n",
    "                pitch_text = \"Head: CENTER\"\n",
    "\n",
    "    # --- ê²°ê³¼ ì¶œë ¥ ---\n",
    "    display_text = f\"{gaze_text} | {pitch_text}\"\n",
    "    cv2.putText(frame, display_text, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    for i, (k, v) in enumerate(gaze_duration_dict.items()):\n",
    "        cv2.putText(frame, f\"{k}: {v:.1f}s\", (30, 100 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"MediaPipe Gaze & Head Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# ë§ˆì§€ë§‰ ì‹œì„  ëˆ„ì \n",
    "if gaze_state:\n",
    "    gaze_duration_dict[gaze_state] += time.time() - gaze_start_time\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\nì´ ì‹œì„  ìœ ì§€ ì‹œê°„:\")\n",
    "for direction, duration in gaze_duration_dict.items():\n",
    "    print(f\"{direction}: {duration:.2f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e80a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Calibration] eye_opening_avg=0.0195, up<0.0176, down>0.0215\n",
      "\n",
      "ì´ ì‹œì„  ìœ ì§€ ì‹œê°„:\n",
      "UP: 57.83ì´ˆ\n",
      "DOWN: 93.58ì´ˆ\n",
      "CENTER: 70.07ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "# ìœ—ëˆˆêº¼í’€ê³¼ ì•„ë« ëˆˆêº¼í’€ì˜ ê±°ë¦¬ ì°¨ì´ -> ëœ¨ë©´ ì¢ì•„ì§€ê³ , ê°ìœ¼ë©´ ê¸¸ì–´ì§.\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "# 3D ëª¨ë¸ í¬ì¸íŠ¸ (solvePnPìš©)\n",
    "model_points = np.array([\n",
    "    (0.0, 0.0, 0.0),\n",
    "    (0.0, -63.6, -12.5),\n",
    "    (-43.3, 32.7, -26.0),\n",
    "    (43.3, 32.7, -26.0),\n",
    "    (-28.9, -28.9, -24.1),\n",
    "    (28.9, -28.9, -24.1)\n",
    "], dtype=np.float32)\n",
    "\n",
    "# ì‹œì„  ìœ ì§€ ì‹œê°„ ì¸¡ì •ìš©\n",
    "gaze_state = None\n",
    "gaze_start_time = time.time()\n",
    "gaze_duration_dict = {\"UP\": 0.0, \"DOWN\": 0.0, \"CENTER\": 0.0}\n",
    "\n",
    "# ì¹´ë©”ë¼ ì„¤ì •\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def get_camera_matrix(w, h):\n",
    "    focal_length = w\n",
    "    center = (w / 2, h / 2)\n",
    "    return np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "# --- ëˆˆêº¼í’€ ê¸°ë°˜ ìë™ ë³´ì •ìš© ---\n",
    "calibration_openings = []\n",
    "frame_idx = 0\n",
    "is_calibrated = False\n",
    "up_thresh = 0.0\n",
    "down_thresh = 1.0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w = frame.shape[:2]\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    gaze_text = \"Gaze: Unknown\"\n",
    "    pitch_text = \"Head: Unknown\"\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "        # --- ì‹œì„  ë¶„ì„ (ëˆˆêº¼í’€ ì—´ë¦¼ ì •ë„) ---\n",
    "        eye_top_id = 159\n",
    "        eye_bottom_id = 145\n",
    "        iris_id = 468\n",
    "\n",
    "        eye_top = landmarks[eye_top_id].y\n",
    "        eye_bottom = landmarks[eye_bottom_id].y\n",
    "        eye_opening = abs(eye_top - eye_bottom)\n",
    "\n",
    "        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ì´ˆê¸° 30í”„ë ˆì„ í‰ê· )\n",
    "        if not is_calibrated:\n",
    "            calibration_openings.append(eye_opening)\n",
    "            frame_idx += 1\n",
    "            if frame_idx == 30:\n",
    "                avg_opening = np.mean(calibration_openings)\n",
    "                up_thresh = avg_opening * 0.9\n",
    "                down_thresh = avg_opening * 1.1\n",
    "                is_calibrated = True\n",
    "                print(f\"[Calibration] eye_opening_avg={avg_opening:.4f}, up<{up_thresh:.4f}, down>{down_thresh:.4f}\")\n",
    "            gaze_direction = \"CENTER\"\n",
    "        else:\n",
    "            if eye_opening < up_thresh:\n",
    "                gaze_direction = \"DOWN\"\n",
    "            elif eye_opening > down_thresh:\n",
    "                gaze_direction = \"UP\"\n",
    "            else:\n",
    "                gaze_direction = \"CENTER\"\n",
    "\n",
    "        gaze_text = f\"Gaze: {gaze_direction}\"\n",
    "\n",
    "        # ì‹œì„  ìœ ì§€ ì‹œê°„ ì¸¡ì •\n",
    "        now = time.time()\n",
    "        if gaze_direction != gaze_state:\n",
    "            if gaze_state:\n",
    "                gaze_duration_dict[gaze_state] += now - gaze_start_time\n",
    "            gaze_start_time = now\n",
    "            gaze_state = gaze_direction\n",
    "\n",
    "        # --- ëœë“œë§ˆí¬ ì‹œê°í™” ---\n",
    "        def draw_landmark_circle(idx, color):\n",
    "            cx = int(landmarks[idx].x * w)\n",
    "            cy = int(landmarks[idx].y * h)\n",
    "            cv2.circle(frame, (cx, cy), 4, color, -1)\n",
    "\n",
    "        draw_landmark_circle(eye_top_id, (0, 255, 0))      # ìœ—ëˆˆêº¼í’€ ì´ˆë¡\n",
    "        draw_landmark_circle(eye_bottom_id, (0, 0, 255))   # ì•„ë«ëˆˆêº¼í’€ ë¹¨ê°•\n",
    "        draw_landmark_circle(iris_id, (255, 0, 0))         # ë™ê³µ íŒŒë‘\n",
    "\n",
    "        # --- ê³ ê°œ ë°©í–¥(pitch) ë¶„ì„ ---\n",
    "        image_points = np.array([\n",
    "            [landmarks[1].x * w, landmarks[1].y * h],\n",
    "            [landmarks[152].x * w, landmarks[152].y * h],\n",
    "            [landmarks[33].x * w, landmarks[33].y * h],\n",
    "            [landmarks[263].x * w, landmarks[263].y * h],\n",
    "            [landmarks[78].x * w, landmarks[78].y * h],\n",
    "            [landmarks[308].x * w, landmarks[308].y * h]\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        camera_matrix = get_camera_matrix(w, h)\n",
    "        dist_coeffs = np.zeros((4, 1))\n",
    "\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            model_points, image_points, camera_matrix, dist_coeffs\n",
    "        )\n",
    "\n",
    "        if success:\n",
    "            rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "            pitch = np.degrees(np.arcsin(-rotation_matrix[2][1]))\n",
    "\n",
    "            if pitch < -15:\n",
    "                pitch_text = \"Head: UP\"\n",
    "            elif pitch > 5:\n",
    "                pitch_text = \"Head: DOWN\"\n",
    "            else:\n",
    "                pitch_text = \"Head: CENTER\"\n",
    "\n",
    "    # --- ê²°ê³¼ ì¶œë ¥ ---\n",
    "    display_text = f\"{gaze_text} | {pitch_text}\"\n",
    "    cv2.putText(frame, display_text, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    for i, (k, v) in enumerate(gaze_duration_dict.items()):\n",
    "        cv2.putText(frame, f\"{k}: {v:.1f}s\", (30, 100 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"MediaPipe Gaze & Head Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# ë§ˆì§€ë§‰ ì‹œì„  ëˆ„ì \n",
    "if gaze_state:\n",
    "    gaze_duration_dict[gaze_state] += time.time() - gaze_start_time\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\nì´ ì‹œì„  ìœ ì§€ ì‹œê°„:\")\n",
    "for direction, duration in gaze_duration_dict.items():\n",
    "    print(f\"{direction}: {duration:.2f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e114ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Calibration] eye_opening_avg=0.0212, up>0.0234, down<0.0191\n",
      "\n",
      "ì´ ì‹œì„  ìœ ì§€ ì‹œê°„:\n",
      "UP: 34.96ì´ˆ\n",
      "DOWN: 47.14ì´ˆ\n",
      "CENTER: 47.76ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "# | ì¡°ê±´                                  | ë™ì‘                     |\n",
    "# | ------------------------------------ | ------------------------ |\n",
    "# | ëˆˆì´ ê±°ì˜ ë‹«í˜”ì„ ë•Œ (`eye_opening < 0.015`) | ê¹œë¹¡ì„ìœ¼ë¡œ íŒë‹¨ â†’ ì´ì „ ì‹œì„  ìƒíƒœ ìœ ì§€ |\n",
    "# | ê¹œë¹¡ì„ ì§í›„ ì¼ì • í”„ë ˆì„(`5 frame`) ë™ì•ˆ      | ì—¬ì „íˆ ì´ì „ ì‹œì„  ìƒíƒœ ìœ ì§€ (ì¿¨ë‹¤ìš´)  |\n",
    "# -> ê´œì°®ê²Œ ë‚˜ì˜¤ëŠ” í¸ -> ìˆ˜ì¹˜ ì¢€ ì¡°ì ˆí•´ì•¼í•  ë“¯\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe ì´ˆê¸°í™”\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "# 3D ëª¨ë¸ í¬ì¸íŠ¸ (solvePnPìš©)\n",
    "model_points = np.array([\n",
    "    (0.0, 0.0, 0.0),\n",
    "    (0.0, -63.6, -12.5),\n",
    "    (-43.3, 32.7, -26.0),\n",
    "    (43.3, 32.7, -26.0),\n",
    "    (-28.9, -28.9, -24.1),\n",
    "    (28.9, -28.9, -24.1)\n",
    "], dtype=np.float32)\n",
    "\n",
    "# ì‹œì„  ìœ ì§€ ì‹œê°„ ì¸¡ì •ìš©\n",
    "gaze_state = None\n",
    "gaze_start_time = time.time()\n",
    "gaze_duration_dict = {\"UP\": 0.0, \"DOWN\": 0.0, \"CENTER\": 0.0}\n",
    "\n",
    "# ì¹´ë©”ë¼ ì„¤ì •\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def get_camera_matrix(w, h):\n",
    "    focal_length = w\n",
    "    center = (w / 2, h / 2)\n",
    "    return np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "# --- ëˆˆêº¼í’€ ê¸°ë°˜ ìë™ ë³´ì •ìš© ---\n",
    "calibration_openings = []\n",
    "frame_idx = 0\n",
    "is_calibrated = False\n",
    "up_thresh = 0.0\n",
    "down_thresh = 1.0\n",
    "\n",
    "# --- ê¹œë¹¡ì„ ê°ì§€ìš© ---\n",
    "blink_threshold = 0.015\n",
    "blink_cooldown_frames = 5\n",
    "blink_cooldown_counter = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w = frame.shape[:2]\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    gaze_text = \"Gaze: Unknown\"\n",
    "    pitch_text = \"Head: Unknown\"\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "        # --- ì‹œì„  ë¶„ì„ (ëˆˆêº¼í’€ ì—´ë¦¼ ì •ë„) ---\n",
    "        eye_top_id = 159\n",
    "        eye_bottom_id = 145\n",
    "        iris_id = 468\n",
    "\n",
    "        eye_top = landmarks[eye_top_id].y\n",
    "        eye_bottom = landmarks[eye_bottom_id].y\n",
    "        eye_opening = abs(eye_top - eye_bottom)\n",
    "\n",
    "        # ê¹œë¹¡ì„ ì²˜ë¦¬\n",
    "        if eye_opening < blink_threshold:\n",
    "            blink_cooldown_counter = blink_cooldown_frames\n",
    "            gaze_direction = gaze_state  # ê¹œë¹¡ì¼ ë•ŒëŠ” ì´ì „ ìƒíƒœ ìœ ì§€\n",
    "        elif blink_cooldown_counter > 0:\n",
    "            blink_cooldown_counter -= 1\n",
    "            gaze_direction = gaze_state  # ì¿¨ë‹¤ìš´ ë™ì•ˆë„ ìœ ì§€\n",
    "        else:\n",
    "            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ì´ˆê¸° 30í”„ë ˆì„ í‰ê· )\n",
    "            if not is_calibrated:\n",
    "                calibration_openings.append(eye_opening)\n",
    "                frame_idx += 1\n",
    "                if frame_idx == 30:\n",
    "                    avg_opening = np.mean(calibration_openings)\n",
    "                    up_thresh = avg_opening * 1.1\n",
    "                    down_thresh = avg_opening * 0.9\n",
    "                    is_calibrated = True\n",
    "                    print(f\"[Calibration] eye_opening_avg={avg_opening:.4f}, up>{up_thresh:.4f}, down<{down_thresh:.4f}\")\n",
    "                gaze_direction = \"CENTER\"\n",
    "            else:\n",
    "                if eye_opening > up_thresh:\n",
    "                    gaze_direction = \"UP\"\n",
    "                elif eye_opening < down_thresh:\n",
    "                    gaze_direction = \"DOWN\"\n",
    "                else:\n",
    "                    gaze_direction = \"CENTER\"\n",
    "\n",
    "        gaze_text = f\"Gaze: {gaze_direction}\"\n",
    "\n",
    "        # ì‹œì„  ìœ ì§€ ì‹œê°„ ì¸¡ì •\n",
    "        now = time.time()\n",
    "        if gaze_direction != gaze_state:\n",
    "            if gaze_state:\n",
    "                gaze_duration_dict[gaze_state] += now - gaze_start_time\n",
    "            gaze_start_time = now\n",
    "            gaze_state = gaze_direction\n",
    "\n",
    "        # --- ëœë“œë§ˆí¬ ì‹œê°í™” ---\n",
    "        def draw_landmark_circle(idx, color):\n",
    "            cx = int(landmarks[idx].x * w)\n",
    "            cy = int(landmarks[idx].y * h)\n",
    "            cv2.circle(frame, (cx, cy), 4, color, -1)\n",
    "\n",
    "        draw_landmark_circle(eye_top_id, (0, 255, 0))      # ìœ—ëˆˆêº¼í’€ ì´ˆë¡\n",
    "        draw_landmark_circle(eye_bottom_id, (0, 0, 255))   # ì•„ë«ëˆˆêº¼í’€ ë¹¨ê°•\n",
    "        draw_landmark_circle(iris_id, (255, 0, 0))         # ë™ê³µ íŒŒë‘\n",
    "\n",
    "        # --- ê³ ê°œ ë°©í–¥(pitch) ë¶„ì„ ---\n",
    "        image_points = np.array([\n",
    "            [landmarks[1].x * w, landmarks[1].y * h],\n",
    "            [landmarks[152].x * w, landmarks[152].y * h],\n",
    "            [landmarks[33].x * w, landmarks[33].y * h],\n",
    "            [landmarks[263].x * w, landmarks[263].y * h],\n",
    "            [landmarks[78].x * w, landmarks[78].y * h],\n",
    "            [landmarks[308].x * w, landmarks[308].y * h]\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        camera_matrix = get_camera_matrix(w, h)\n",
    "        dist_coeffs = np.zeros((4, 1))\n",
    "\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            model_points, image_points, camera_matrix, dist_coeffs\n",
    "        )\n",
    "\n",
    "        if success:\n",
    "            rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "            pitch = np.degrees(np.arcsin(-rotation_matrix[2][1]))\n",
    "\n",
    "            if pitch < -15:\n",
    "                pitch_text = \"Head: UP\"\n",
    "            elif pitch > 5:\n",
    "                pitch_text = \"Head: DOWN\"\n",
    "            else:\n",
    "                pitch_text = \"Head: CENTER\"\n",
    "\n",
    "    # --- ê²°ê³¼ ì¶œë ¥ ---\n",
    "    display_text = f\"{gaze_text} | {pitch_text}\"\n",
    "    cv2.putText(frame, display_text, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    for i, (k, v) in enumerate(gaze_duration_dict.items()):\n",
    "        cv2.putText(frame, f\"{k}: {v:.1f}s\", (30, 100 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"MediaPipe Gaze & Head Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# ë§ˆì§€ë§‰ ì‹œì„  ëˆ„ì \n",
    "if gaze_state:\n",
    "    gaze_duration_dict[gaze_state] += time.time() - gaze_start_time\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\nì´ ì‹œì„  ìœ ì§€ ì‹œê°„:\")\n",
    "for direction, duration in gaze_duration_dict.items():\n",
    "    print(f\"{direction}: {duration:.2f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d101c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a8830d275db2f5d56d95cb53fb469d1ca33211086217bbd3243957b61467a0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
